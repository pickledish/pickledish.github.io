---
layout: post
title: "Maybe You Don't Actually Want Single Tenancy"
permalink: /posts/tenancy/
---

This is a letter to, I dunno, software engineering leaders I guess, the folks who sign contracts, from an IC who worked at one of the companies they might be purchasing services from.

It's about "tenancy" -- the number of customers whose data a SaaS vendor might colocate and serve from the same machine (or set of machines). In a multitenant system, many customers' data are colocated, whereas in a single-tenant system, you know that your data is handled by servers which only ever touch _your_ data.

That latter one! Sounds like a good thing, right?

Well... sometimes. Let's talk about it.

---

For some SaaS businesses, multitenant is the default, with single-tenancy being an option on the table if you ask for it and pay enough (i.e. part of that nebulous "enterprise" pricing tier). I am aware of a few reasons an engineering leader might have for wanting a vendor to manage their data in a single-tenant way:

* **Security** -- less chance of us being collateral damage if data's physically somewhere else
* **Performance** -- we don't want other customers' queries slowing us down
* **Reliability** -- bad data from (or surgical operations for) other customers can't affect us

I'm not really a security or compliance guy, so I won't address that first bullet point -- there's probably validity there, and I don't mean to argue that.

The other two, however, are some of those things that sound a lot better on paper than they often are in real life.

---

Let me talk about why through an example, a software system similar-but-simplified from what we used for the metrics data pipeline while I was at Datadog.

Imagine a "cell" to be a complete system for data ingest, storage, and querying:

<img src="/assets/tenancy/cell-1.png" class="halfwidthimage">

And, a big SaaS vendor might maintain many of these cells (useful for load distribution, and often necessary when handling silly amounts of data):

<img src="/assets/tenancy/cell-2.png" class="halfwidthimage">

Like with shards of a database, a given customer's data will fall into a single cell, with some cells handing many customers' data and some handling only one customer's data:

<img src="/assets/tenancy/cell-3.png" class="halfwidthimage">

In a system like the above, there's a good chance that requesting your data be moved from a large multitenant cell to your own new single-tenant cell will end up with you paying more money for **less performance** and **less reliability**.

---

Both of these potential consequences come from the realities of running a big system.

The result might be **less performant** simply because of resource allocation. Chances are, the aforementioned "large multitenant cell" has a lot of compute or storage resources allocated towards it -- servers with many cores, fast expensive SSDs, etc -- to handle the load. Thus, a choice to move your data to a significantly-smaller single-tenant cell would come with a corresponding reduction in available resources.

Now, wouldn't that not be a big deal, since you're no longer fighting other customers for data-ingest throughput and query runners, so all _your_ stuff would be faster? Maybe, if things scale down perfectly that way. But it's also possible, likely even, that the "reduction in resources" results in your cell getting smaller (slower) hardware than the big cells, or you'd otherwise be affected adversely by differences in things like caching. After all, the business's developers are trying to optimize for average or 99th-percentile query latency, but your queries are now very different from everyone else's.

---

And, the result might be **less reliable**, too. Being a part of a very large multitenant cell comes with a certain "safety in numbers".

What I mean is, if the service's developers want to roll out a big change slowly across the cells, it stands to reason that they'd test the waters on the less-scary, smaller-blast-radius single tenant cells first, making them effectively unwilling beta-testers.

Or, to think about it a different way -- maybe, in the database layer of the cells, our developers on call like to keep a certain amount of headroom in the servers' available disk space, say 20%. Now, the very large multitenant cells need to have huge attached disks, right? So for them, that "20% headroom" might manifest as hundreds or even thousands of gigabytes.

But, your new single-tenant cell is something like 1/100 the size of that large multitenant cell, and would then come with 1/100 the storage, too -- and that "hundreds or thousands" is now single-digit gigabytes of headroom. If a single-customer misconfiguration (one which results in duplicate data being ingested) would have allowed 6 hours for fixing before blowing up the storage disks in the multitenant cell, that same misconfiguration in your smaller single-tenant cell leads to that disaster in just _a couple of minutes_.

---

Anyway, that's the blog post! It's a perspective that's a bit nonintuitive, and one I don't see around very much, so I thought it might be useful to share. As always, please feel free to reach out if you liked this **or** (especially) if you disagree with something and believe me to be spreading misinformation in some way.

Thanks for reading!
