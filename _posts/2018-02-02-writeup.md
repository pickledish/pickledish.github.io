---
layout: post
title: "MC Time Series Subset Search"
permalink: /posts/mcwriteup/
---

## The Goal

The goal of the program is to take in a set of time-series data, in our current case stock closing prices for $n$ different stocks, and use a Monte Carlo analysis to determine the "best" subset of those $n$ stocks. In our case, "best" is defined to be the subset that yields a high return (during the analyzed time period of course, but hopefully holding for the future as well) while minimizing risk.

## What We Currently Do

### The Setup

On the left of the screen, there are several input fields:

- `Iterations`, which is an integer representing how thoroughly the algorithm investigates "best" subset possibilities
- `Anneal`, which is a real number used to determine how often "bad" states are accepted despite their "bad"-ness
- `Risk Free`, a real number in $[0,1]$ which adjusts the "return" vector to more accurately reflect real-life earnings
- `Stock Codes`, a comma-separated list of U.S. ticker names to fetch time-series price data of
- `Start` and `End`, the date range in which to fetch data for the aforementioned ticker names
- `Sample`, the frequency at which to sample the closing prices (every day, every month, every year, and so on)

Upon clicking "Submit", we use the Quandl API to fetch up-to-date closing prices of the tickers based on `Stock Codes` across the given date range `Start` to `End`. Then, we arrange the data into a matrix $T$, with row-vectors being the data from a single stock, thus getting an $n \times m$ matrix where $m$ is the number of samples based on `Sample`. From $T$, we create two new matrices:

1. $C$, the correlation matrix, which is of dimension $n \times n$ and where entry $C_{ij}$ represents the correlation of the time-series row vectors of stock $i$ and stock $j$
2. $D$, a special matrix also of dimension $n \times n$, where $D$ is created using the covariance matrix through a process detailed below.

> Note: $C$ and $D$ are made deliberately so that they act in roughly the same way with respect to the algorithm, and either one can be dropped in to act as the driving matrix of the algorithm. We'll just use $C$ below for simplicity.

Using our time-series matrix $T$, we also find $b$, the "average return" vector. Each of the $n$ components of $b$ is calculated as follows:

$$ b_i = n \cdot \left(\frac{T_{in}}{T_{i1}}\right)^{1\,/\,(n - 1)}$$

Thus, equipped with an $n \times n$ matrix $C$ and a size-$n$ vector $b$, we can solve the equation $Cx = b$ to find the solution vector $x$, and then normalize $x$ to get $x'$. We then define the tuple $(C, x', b)$ to be a "state" of our algorithm, and we can rephrase the Goal at the top as follows: we're trying to find the state with the highest score, where the score of a state is defined to be:

$$ s(C, x', b) = \frac{
	\sum_{i=1}^n{b_i x_i'}
}{
	\sum_{i=1}^n{C_{ii}^2x_i'} + 
	\sum_{i=1}^n \sum_{j \neq i}{C_{ij} x_i' x_j'}
} $$

### Finding $D$

Using the correlation matrix $C$ gives good results, but is not an ideal solution, since the main diagonal of $C$ is all $1$, and thus it contains no information about a stock's individual fluctuations (interpreted as risk). To combat this, we originally tried to use the covariance matrix $V$ of the time-series data instead, as in that case $V_{ii}$ the variance of an individual stock would be a factor. However, this yielded very poor results (for unclear reasons). Thus, we instead take the correlation matrix $C$ and modify its diagonal entries to obtain a suitable matrix $D$.

First, we find $\beta$ as follows:

- Hello

	- Hello
	  How are you
	- Hello
	- Hello

$$ \beta = \max \left\{ \,\sum_{j = 1,\, j \neq i}^n{C_{ij}} \quad \text{ for all } i \in \{1,2,\ldots,n\} \right\} $$

We calculate this value because we'd like $D$ to be positive definite, which is the case if and only if each entry on the diagonal is greater than the sum of all other values in its row. Therefore, $\beta$ is a "safe" scalar in that, if each entry on the diagonal of $C$ is at least $1$ and we multiply each of those entries by $\beta$, the result will necessarily be positive definite.

Then, we can construct $D$. First, $D$ begins as a copy of $V$ the covariance matrix, except that each diagonal entry $D_{ii}$ is "normalized" (divided by the square of the mean of row $i$) and then "scaled" (divided by the minimum entry on the diagonal, so that the smallest diagonal entry is $1$ and all others are larger). Then, we copy into $D$ all of the non-main-diagonal entries from $C$ the correlation matrix, so that $D$ is _mostly_ the correlation matrix but with the main diagonal of $V$. Finally, we multiply the diagonal entries of $D$ by $\beta$, to ensure that $D$ is positive definite.

### The Algorithm

Now, this part would be simple if $n$ were always small, as we could just iterate over all possible subsets of tickers to find the one whose state gave the highest score. However, when $n$ is large, the fact that there are $2^n$ such subsets makes this solution intractable, so we're forced to do a probabilistic search. The search goes as follows:

For each integer $i$ from $1$ to $n$, we'll randomly choose $i$ integers from $\\{1,2,\ldots,n\\}$ and "strike out" the rows and columns from $C$ corresponding to those integers to create $C'$ (as one would do in the process of cofactor expansion), and similarly the respective components of $b$ to create $b'$. Let the set of the indices of the remaining rows/columns in $C'$ be called $R$. We calculate the score of the state gotten from $C'$ and $b'$, and call that the "baseline".

Then, we repeat the process to get a new $C'$ and $b'$ based on a different set $R$ of remaining vertices, and if the score of that state is higher than the baseline, then it _becomes_ the new baseline. This process is repeated `Iterations` times, and because we keep track of each $C'$, $b'$ and $R$, we can create histograms of which rows/columns are the most common to appear in "victorious" states (ones that beat the previous baseline and became the new baseline).

> Note: Because this is an annealing algorithm, we also sometimes accept states whose scores are _lower_ than the baseline. The probability of this happening is given by whether $e^{ad} > \mu$, where $a$ is `Anneal`, $d$ is the difference between the scores, and $\mu$ is uniformly random on $(0,1)$.

Then to finish, once these histograms have been made (and the main loop of the algorithm is done), we find the best-guess optimum for each $i$ from $1$ to $n$, where this optimum is defined to be the tallest $i$ bars (the most common $i$ rows/columns in "victorious" states) in the specific histogram which corresponds to states where $\|R\| = i$. These best-guess optima are all sets of indices, each with a different size -- the smallest one only has one element (presumably the index of the best stock) and the largest is $\\{1,2,\ldots,n\\}$. Then, for each such set $S$, we get the matrix $C'$ and vector $b'$ from $C$ and $b$ but with only indices from $S$ included, like before, and find the score of that state. The state that achieves the highest score in this process is the "best" subset of the $n$ stocks.

### Comparisons

At the end of the process, we have a "best" subset of rows to use according to the algorithm using matrix $C$. Then, we run the whole thing again, keeping all things the same except for using $D$ in place of $C$, and save that output's "best" subset of rows as well. Then, at the bottom of the page's rendered output, there's a section labeled **check the difference**. Here, one can enter a time period for data collection, and directly compare how the subset from $C$ and the subset from $D$ would perform in terms of returns over time.

In addition, the area just above this (labeled **compare portfolio performance**) allows for similar performance comparisons between the subset from $C$ and a uniform portfolio, the S&P 500, and a custom portfolio.

## Areas for Improvement

- Make the website "responsive", so the interface is still usable on a smartphone or tablet
- Very much of this process could be (and possibly needs to be) parallelized, though that would likely require a move away from Python as the implementation language
- Figure out how hard it would be to add ETF functionality to the program, instead of only stocks
- Similarly, we could add stocks from different countries
- More (or different) graphs, maybe with JS, make the data easier to visualize

